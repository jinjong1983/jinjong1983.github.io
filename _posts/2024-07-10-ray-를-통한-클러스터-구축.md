---
title: "ray 를 통한 클러스터 구축 "
date: 2024-07-10
description: "예전에 병렬처리 찾아볼때 ray라는 opensource framework가 있었다.구글을 뒤져봤지만. 꽤많은 자료가 나왔으나 windows 환경의 구성은자료가 많지 않아서 작성해본다.https&#x3A;//github.com/ray-project/rayhttps&#x3"
tags: ["ray", "병렬처리", "분산처리", "파이썬"]
categories: ["Tech Note", "Python"]
---
![](/assets/img/posts/abe58411-7f21-4922-9454-ad932f524d41-image.jpg)

예전에 병렬처리 찾아볼때 ray라는 opensource framework가 있었다.
구글을 뒤져봤지만. 꽤많은 자료가 나왔으나 windows 환경의 구성은자료가 많지 않아서 작성해본다.

https://github.com/ray-project/ray
https://docs.ray.io/en/latest/index.html
![](/assets/img/posts/8ffc0f71-0ac2-475a-9e63-0945e63581c2-image.png)

# 1. 단일 프로세스 & gil
python 에서 특정 패키지가 여러개의 프로세스를 사용하는 경우는 있으나 기본적으로 일반적인 코드로 사용한다면 단일 프로세스에서 돌아간다. 더군다나 gil 까지 있으니 일반 코드 외에는 성능적인 부분은 떨어진다.

numpy 라던가 pypy 라던가 뭐 다양한 테크닉적인 요소의 해결책이 시도되었으나 이는 테크닉적인 부분이다. 네트웍 io 의 경우는 비동기를 태워서 개선을 시도해본다 하지만, cpu 연산이 들어가는 부분을 경험해본다면 누구나 병렬처리 멀티프로세싱에 대한 생각을 해보았을것이다.

# 2. celery & gunicorn & ray
셀러리와 구니콘은 이를 해결하는 사례의 좋은 예시이다. n개의 워커를 통해서 개선을 시도한다. ray 는 기존 코드의 변경사항을 최소화 한 형태로 아주 쉽게 분산처리를 할 수 있으며 유연한 워커의 확장을 가능하게 해준다. 같은 처리를 multiprocessing 으로 사용하는 것보다 훨씬 유리한 성능을 보여준다.
![](/assets/img/posts/8ddeb877-e23b-40a4-95e4-e66b724a47ce-image.png)

# 3. ray 설치, 실행 standalone
- ray는 아주 간단하게 설치할 수 있다.
```
# 레이 설치(기본)
pip install ray

# 레이 설치시에 몇가지 추가기능을 쓰고 싶다면 default 를 추가한다. 대쉬보드사용에 필요하다
pip install ray[default]
```
설치한 레이를 실행하는건 두가지 형태가 있다.
- standalone 의 형태로 .py 프로그램에서 실행하고 종료할 수 있다.
- 아래의 코드는 파이참에서 패키지 설치하고 돌리면 걍 돌아간다.
```
import ray

# Ray 초기화 및 실행
ray.init()

@ray.remote
def add(x, y):
    return x + y

# 태스크를 병렬로 실행
futures = [add.remote(1, 2), add.remote(3, 4), add.remote(5, 6)]
results = ray.get(futures)

print(f"Results: {results}")

# Ray 종료
ray.shutdown()

```

# 4. ray 설치, 실행 cluster multinode
ray 의 진가는 standalone 에서 multiprocessing 을 없는 패키지로 만들어지는 부분에서도 느낄 수 있지만 황당할 정도로 간단한 클러스터 구성과, 기존코드의 분산처리기능으로의 수정에서 체험할 수 있다.

- 세대의 컴퓨터에서 처리를한다고 가정한다면 세대모두 standalone 과 동일한 형태로의 ray 설치가 필요하다.

- 한대의 노드는 cli 환경에서 head 로 실행하고 나머지 워커노드들은  동일한 실행시에 헤드노드를 연결하는 명령어를 추가하면 된다.

```
# 상세 옵션은 --help 에서 확인할 수 있으며 대쉬보드 를 추가하였다. 
# 1.1.1.1 은 헤드노드의 ip로 수정
ray start --head --node-ip-address=1.1.1.1 --include-dashboard true

# 워커노드 실행시에 헤드의 ip 와 워커의 ip두개를 모두 입력해야한다.
# 6379 포트는 헤드노드의 디폴트 포트이며 지정하여 바꿀 수 있다.
ray start --address=1.1.1.1:6379 --node-ip-address=1.1.1.2

```

- 헤드노드에서 헤드노드 실행, 두대의 워커에서 워커노드를 각각 실행한것 만으로도 클러스터 구성이 완료되었다.
![](/assets/img/posts/67945e9d-fdde-419f-a8c6-21cd7f34d816-image.png)

**주의사항**
- windows 와 mac os 에서의 ray 클러스터 구성은 공식적으로 지원하는 대상이 아니다. 때문에 os환경변수 에 "**RAY_ENABLE_WINDOWS_OR_OSX_CLUSTER=1**" 지정이 필요하다
- 각 클러스터간 자원 공유시에 ray 는 gcs(global control service)라고 하는걸 사용한다. gcs는 redis 로 구성되어서 pip install redis 명령어로 레디스 설치가 필요하다.

**대시보드**
![](/assets/img/posts/c2e6fb09-ed10-4e6d-9c94-0686a6eddfeb-image.png)

![](/assets/img/posts/ca8df8a5-9e30-44aa-80e6-df9b61d0417f-image.png)
- 정상적으로 클러스터간 연결이 되었다면 아래와 같은 클러스터 대쉬보드를 사용할 수 있으며 대쉬보드를 더 잘 사용하기 위하여 몇가지 의존성패키지 설치도 선택하여 진행할 수 있다.

**job**
클러스터 구성이 안정적으로 완료되었다면 job의 제출이 필요하다. 3번항목의 예제에서 ray.init() 코드를 **ray.init(address="1.1.1.1:8265")** 로 수정하고 헤드노드에서 실행한다면 해당 ip:port 를 통해서 클러스터에 구성된 워커에 뿌려지고 실행된다.

**참고**
https://zzsza.github.io/mlops/2021/01/03/python-ray/ (변성윤님블로그)
https://seongyun-dev.tistory.com/53 

**후기**
검색하면 나오는 ray관련포스팅이 전부 ml 쪽인데 데엔의 미래는 duckdb와 ray에 일부 있지 않을까란 생각이 든다.. 아직 공부해야 할게 많지만, 온프레미스 윈도우 환경 클러스터 구성 포스팅은 보지  못했다. 러닝레이 책이나 문서쪽에 모르는내용이 지나칠정도로 많이 있어서 랭체인과 함께 공부해볼 주제의 매우 상단에 올려둘만할 것 같다.